eta = eeta[which(xxx == max(xxx))],
subsample = 0.9
)
pred = predict(mod_xg, newdata = test_data, reshape = TRUE)
pred = as.data.frame(pred)
colnames(pred) = sort(unique(train_label))
pred = apply(pred,1,function(x) colnames(pred)[which.max(x)])
confusion_matrix <- confusionMatrix(factor(as.numeric(test_label), levels = lv), factor(pred, levels = lv), mode = 'prec_recall')
acc <- c(acc, confusion_matrix$overall[1]) %>% as.numeric()
p_value <- c(p_value, confusion_matrix$overall[6]) %>% as.numeric()
result_matrix <- confusion_matrix$byClass %>% as.data.frame()
precision <- rbind(precision, result_matrix$Precision)
recall <- rbind(recall, result_matrix$Recall)
f1 <- rbind(f1, result_matrix$F1)
paste('Random Forest Accuracy :', confusion_matrix$overall[1]) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Random Forest p_value :', confusion_matrix$overall[6]) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Random Forest Class_Result : ') %>% print()
result_matrix %>% print()
}
mean_f1 <- apply(f1, 2, mean) %>% matrix(nrow = 1)
mean_recall <- apply(recall, 2, mean) %>% matrix(nrow = 1)
mean_precition <- apply(precision, 2, mean) %>% matrix(nrow = 1)
mean_acc_p <- c(mean(acc), mean(p_value)) %>% matrix(nrow = 1)
overall_sum = list(mean_precition, mean_recall, mean_f1, mean_acc_p)
return(overall_sum)
}
################################################################################################
# class adaboost
class_adabst_scale <- function(data, order, trans_method){
set.seed(123)
oooorder = sample(1:nrow(data), floor(0.75 * nrow(data)), replace = FALSE)
train_data = data[oooorder,]
train_data = train_data %>% select(-(ncol(train_data) - 1))
train_label = as.factor(train_data[,ncol(train_data)])
train_data = train_data[,-ncol(train_data)]
train_data = train_data
trans_model = preProcess(train_data, method = trans_method)
train_data = predict(trans_model, train_data)
test_data = data[-oooorder,]
test_label <- test_data[,ncol(test_data)]
test_data <- test_data[,-ncol(test_data)]
test_data = predict(trans_model, test_data)
lv = levels(as.factor(unlist(data[ncol(data)])))
coeflearn <- c('Breiman', 'Zhu', 'Freund')
xxx <- c()
train_data = cbind(train_data, train_label)
for (i in coeflearn){
colnames(train_data)[ncol(train_data)] <- 'train_data[, ncol(train_data)]'
model = boosting(`train_data[, ncol(train_data)]` ~ ., data = train_data, boos = TRUE, mfinal = 500, coeflearn = i)
pred = predict(model, newdata = test_data)
acc <- confusionMatrix(factor(pred$class, levels = lv), factor(test_label, levels = lv), mode = 'prec_recall')$overall[1]
xxx <- c(xxx, acc)
}
paste('The Optimal Coeflearn is :', coeflearn[which(xxx == max(xxx))]) %>% print()
acc = c()
p_value = c()
precision = c()
recall = c()
f1 = c()
for(i in 1:5){
print(i)
train_data = data[-unlist(order[i]),]
train_label = as.factor(train_data[,ncol(train_data)])
train_data = train_data[,-ncol(train_data)]
test_data = data[unlist(order[i]),]
test_label = test_data[,ncol(test_data)]
test_data = test_data[,-ncol(test_data)]
trans_model = preProcess(train_data, method = trans_method)
train_data = predict(trans_model, train_data)
test_data = predict(trans_model, test_data)
train_data = cbind(train_data, train_label)
colnames(train_data)[ncol(train_data)] <- 'train_data[, ncol(train_data)]'
modd = boosting(`train_data[, ncol(train_data)]` ~., data = train_data, boos = TRUE, mfinal = 500, coeflearn = coeflearn[which(xxx == max(xxx))][1])
pred = predict(modd, newdata = test_data)
confusion_matrix <- confusionMatrix(factor(pred$class, levels = lv), factor(test_label, levels = lv), mode = 'prec_recall')
acc <- confusion_matrix$overall[1]
p_value <- c(p_value, confusion_matrix$overall[6]) %>% as.numeric()
result_matrix <- confusion_matrix$byClass %>% as.data.frame()
precision <- rbind(precision, result_matrix$Precision)
recall <- rbind(recall, result_matrix$Recall)
f1 <- rbind(f1, result_matrix$F1)
paste('Adaboost Accuracy :', confusion_matrix$overall[1]) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Adaboost p_value :', confusion_matrix$overall[6]) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Adaboost Class_Result : ') %>% print()
result_matrix %>% print()
}
mean_f1 <- apply(f1, 2, mean) %>% matrix(nrow = 1)
mean_recall <- apply(recall, 2, mean) %>% matrix(nrow = 1)
mean_precition <- apply(precision, 2, mean) %>% matrix(nrow = 1)
mean_acc_p <- c(mean(acc), mean(p_value)) %>% matrix(nrow = 1)
overall_sum = list(mean_precition, mean_recall, mean_f1, mean_acc_p)
return(overall_sum)
}
############################################################################
keras_class_scale <- function(data, order, trans_method){
lv = levels(as.factor(unlist(data[ncol(data)])))
acc = c()
p_value = c()
precision = c()
recall = c()
f1 = c()
for(i in 1:5){
train_data = data[-unlist(order[i]),]
train_label = train_data[,ncol(train_data)]
train_data = train_data[,-ncol(train_data)]
test_data = data[unlist(order[i]),]
test_label = test_data[,ncol(test_data)]
test_data = test_data[,-ncol(test_data)]
trans_model = preProcess(train_data, method = trans_method)
train_data = predict(trans_model, train_data)
test_data = predict(trans_model, test_data)
train_data <- cbind(train_data, train_label) %>% as.data.frame()
test_data <- cbind(test_data, test_label) %>% as.data.frame()
colnames(train_data)[ncol(train_data)] <- colnames(data)[ncol(data)]
colnames(test_data)[ncol(test_data)] <- colnames(data)[ncol(data)]
pred_name = colnames(train_data)[ncol(train_data)]
train_file <- paste0(tempdir(), "/train.csv")
write.csv(train_data, train_file, row.names = FALSE)
test_file_to_eval <- paste0(tempdir(), "/eval.csv")
write.csv(test_data, test_file_to_eval, row.names = FALSE)
class <- model_structured_data_classifier(max_trials = 20) %>%
fit(
train_file,
pred_name,
validation_data = list(test_file_to_eval, pred_name)
)
pred <- class %>% predict(test_file_to_eval) %>% as.factor()
true = unlist(test_data[ncol(test_data)]) %>% as.numeric() %>% as.factor()
confusion_matrix <- confusionMatrix(factor(pred, levels = lv), factor(true, levels = lv))
acc <- c(acc, confusion_matrix$overall[1]) %>% as.numeric()
p_value <- c(p_value, confusion_matrix$overall[6]) %>% as.numeric()
result_matrix <- confusion_matrix$byClass %>% as.data.frame()
precision <- rbind(precision, result_matrix$Precision)
recall <- rbind(recall, result_matrix$Recall)
f1 <- rbind(f1, result_matrix$F1)
paste('Keras_Class Accuracy :', confusion_matrix$overall[1]) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Keras_Class p_value :', confusion_matrix$overall[6]) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Keras_Class Class_Result : ') %>% print()
result_matrix %>% print()
}
mean_f1 <- apply(f1, 2, mean) %>% matrix(nrow = 1)
mean_recall <- apply(recall, 2, mean) %>% matrix(nrow = 1)
mean_precition <- apply(precision, 2, mean) %>% matrix(nrow = 1)
mean_acc_p <- c(mean(acc), mean(p_value)) %>% matrix(nrow = 1)
overall_sum = list(mean_precition, mean_recall, mean_f1, mean_acc_p)
return(overall_sum)
}
#######################
print_result <- function(KNN_result, naive_result, svm_result, rf_result, xgb_result, adab_result, keras_result){
# build a empty data frame to save the result
acc_and_p <- matrix(ncol = 2, nrow = 0)
colnames(acc_and_p) <- c('Accuracy', 'p-value')
acc_and_p <- rbind(acc_and_p, unlist(KNN_result[4]))
acc_and_p <- rbind(acc_and_p, unlist(naive_result[4]))
acc_and_p <- rbind(acc_and_p, unlist(svm_result[4]))
acc_and_p <- rbind(acc_and_p, unlist(rf_result[4]))
acc_and_p <- rbind(acc_and_p, unlist(xgb_result[4]))
acc_and_p <- rbind(acc_and_p, unlist(adab_result[4]))
acc_and_p <- rbind(acc_and_p, unlist(keras_result[4]))
rownames(acc_and_p) <- c('KNN', 'Naive_Bayes',
'SVM', 'Random Forest',
'XGBoost', 'Adaboost', 'AutoML')
print(acc_and_p)
# build f1
f1_value <- matrix(ncol = length(unlist(KNN_result[3])), nrow = 0)
f1_value <- rbind(f1_value, unlist(KNN_result[3]))
f1_value <- rbind(f1_value, unlist(naive_result[3]))
f1_value <- rbind(f1_value, unlist(svm_result[3]))
f1_value <- rbind(f1_value, unlist(rf_result[3]))
f1_value <- rbind(f1_value, unlist(xgb_result[3]))
f1_value <- rbind(f1_value, unlist(adab_result[3]))
f1_value <- rbind(f1_value, unlist(keras_result[3]))
rownames(f1_value) <- c('KNN', 'Naive_Bayes',
'SVM', 'Random Forest',
'XGBoost', 'Adaboost', 'AutoML')
print(f1_value)
# build precision
precision_value <- matrix(ncol = length(unlist(KNN_result[1])), nrow = 0)
precision_value <- rbind(precision_value, unlist(KNN_result[1]))
precision_value <- rbind(precision_value, unlist(naive_result[1]))
precision_value <- rbind(precision_value, unlist(svm_result[1]))
precision_value <- rbind(precision_value, unlist(rf_result[1]))
precision_value <- rbind(precision_value, unlist(xgb_result[1]))
precision_value <- rbind(precision_value, unlist(adab_result[1]))
precision_value <- rbind(precision_value, unlist(keras_result[1]))
rownames(precision_value) <- c('KNN', 'Naive_Bayes',
'SVM', 'Random Forest',
'XGBoost', 'Adaboost', 'AutoML')
print(precision_value)
# build recall
recall_value <- matrix(ncol = length(unlist(KNN_result[2])), nrow = 0)
recall_value <- rbind(recall_value, unlist(KNN_result[2]))
recall_value <- rbind(recall_value, unlist(naive_result[2]))
recall_value <- rbind(recall_value, unlist(svm_result[2]))
recall_value <- rbind(recall_value, unlist(rf_result[2]))
recall_value <- rbind(recall_value, unlist(xgb_result[2]))
recall_value <- rbind(recall_value, unlist(adab_result[2]))
recall_value <- rbind(recall_value, unlist(keras_result[2]))
rownames(recall_value) <- c('KNN', 'Naive_Bayes',
'SVM', 'Random Forest',
'XGBoost', 'Adaboost', 'AutoML')
print(recall_value)
scx <- list(acc_and_p, f1_value, precision_value, recall_value)
names(scx) <- c("Accuracy & p-value", "F1-Value", "Precision", "Recall")
return(scx)
}
all_class_together_scaled <- function(data, pos_X, pos_y, method, scale_method){
data <- data_selection(data, pos_X, pos_y, method)
order = data_into_five(data)
KNN_result <- class_KNN_scale(data, order,scale_method)
naive_result <- class_naive_scale(data, order,scale_method)
svm_result <- class_svm_scale(data, order,scale_method)
rf_result <- class_rf_scale(data, order,scale_method)
xgb_result <- xgboost_scale(data, order,scale_method)
adab_result <- class_adabst_scale(data, order,scale_method)
keras_result <- keras_class_scale(data, order,scale_method)
result <- print_result(KNN_result, naive_result, svm_result, rf_result, xgb_result, adab_result, keras_result)
kme_pca(data) %>% print()
return(result)
}
stat_ml <- function(data, pos_X, pos_y, objective, na_method, scale_method = NULL){
method = na_method
trans_method = scale_method
start_time = Sys.time()
if(is.null(scale_method)){
if(objective == 'reg'){results = all_reg_together(data, pos_X, pos_y, na_method)}
else{results = all_class_together(data, pos_X, pos_y, na_method)}
}else{
if(objective == 'reg'){results = all_reg_together_scaled(data, pos_X, pos_y, na_method, trans_method)}
else{results = all_class_together_scaled(data, pos_X, pos_y, na_method, scale_method)}
}
end_time = Sys.time()
warning <- "!!! ALL THE RESULTS HAVE BEEN AVERAGED BASED ON CROSS VALIDATION !!!"
time_diff = end_time - start_time
print(warning)
return(results)
print(time_diff)
}
devtools::install()
devtools::install()
data2 <- read_csv("C:/Users/ft7b6/Downloads/Data_v7_220503_clean2.csv")
colnames(data2) <- paste0('a', colnames(data2))
install.packages("C:/Users/ft7b6/OneDrive/Desktop/Quantlyzer", repos = NULL, type="source")
library(Quantlyzer)
data2 <- read_csv("C:/Users/ft7b6/Downloads/Data_v7_220503_clean2.csv")
colnames(data2) <- paste0('a', colnames(data2))
a2_start = Sys.time()
a2 <- stat_ml(data2,
pos_X = c(7, 10, 11, 12,13,14, 19,20,21:377),
pos_y = 8,
objective = 'reg',
na_method = 'omit',
scale_method = 'scale'
)
a2_end = Sys.time()
b2_start = Sys.time()
b2 <- stat_ml(data2,
pos_X = c(7, 10, 11, 12,13,14, 19,20,21:377),
pos_y = 8,
objective = 'reg',
na_method = 'omit'
)
b2_end = Sys.time()
a2
a2_start
data = data_selection(data,
pos_X = c(7, 10, 11, 12,13,14, 19,20,21:377),
pos_y = 8,
'omit')
data2 <- read_csv("C:/Users/ft7b6/Downloads/Data_v7_220503_clean2.csv")
colnames(data2) <- paste0('a', colnames(data2))
data = data_selection(data,
pos_X = c(7, 10, 11, 12,13,14, 19,20,21:377),
pos_y = 8,
'omit')
order = data_into_five(data)
pearson_cor = c()
xxx = c()
rmseee = c()
maeee = c()
depthh = c()
eeta = c()
lambdaa = c()
mod5_data <- data %>% as.matrix()
set.seed(123)
oooorder = sample(1:nrow(data), floor(0.75 * nrow(data)), replace = FALSE)
train_data = data[oooorder,]
train_label = train_data[,ncol(train_data)]
train_data = train_data[,-ncol(train_data)]
test_data = data[-oooorder,]
test_label = test_data[,ncol(test_data)]
test_data = test_data[,-ncol(test_data)]
trans_model = preProcess(train_data, method = trans_method)
train_data = predict(trans_model, train_data) %>% as.matrix()
test_data = predict(trans_model, test_data)  %>% as.matrix()
for(i in 1:5){
set.seed(123)
print(i)
train_data = data[-unlist(order[i]),] %>% as.data.frame()
test_data = data[unlist(order[i]),] %>% as.data.frame()
pred_name = colnames(train_data)[ncol(train_data)]
train_file <- paste0(tempdir(), "/train.csv")
write.csv(train_data, train_file, row.names = FALSE)
test_file_to_eval <- paste0(tempdir(), "/eval.csv")
write.csv(test_data, test_file_to_eval, row.names = FALSE)
class <- model_structured_data_regressor(max_trials = 20) %>%
fit(
train_file,
pred_name,
validation_data = list(test_file_to_eval, pred_name)
)
pred <- class %>% predict(test_file_to_eval)
true = test_data[,ncol(test_data)] %>% as.numeric()
pearson_cor <- c(pearson_cor, cor(pred, true))
r_2 <- c(r_2, coefficient_of_determination(true, pred))
rmseee <- c(rmseee, rmse(true, pred))
maeee <- c(maeee, mae(true, pred))
paste('Pearson Coefficient Correlation :', pearson_cor) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Coefficient of Determination :', r_2) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('RMSE : ', rmseee) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('MAE : ', maeee) %>% print()
}
set.seed(123)
print(i)
train_data = data[-unlist(order[i]),] %>% as.data.frame()
test_data = data[unlist(order[i]),] %>% as.data.frame()
pred_name = colnames(train_data)[ncol(train_data)]
train_file <- paste0(tempdir(), "/train.csv")
write.csv(train_data, train_file, row.names = FALSE)
test_file_to_eval <- paste0(tempdir(), "/eval.csv")
write.csv(test_data, test_file_to_eval, row.names = FALSE)
class <- model_structured_data_regressor(max_trials = 20) %>%
fit(
train_file,
pred_name,
validation_data = list(test_file_to_eval, pred_name)
)
pred <- class %>% predict(test_file_to_eval)
true = test_data[,ncol(test_data)] %>% as.numeric()
pearson_cor <- c(pearson_cor, cor(pred, true))
r_2 <- c(r_2, coefficient_of_determination(true, pred))
rmseee <- c(rmseee, rmse(true, pred))
maeee <- c(maeee, mae(true, pred))
for(i in 1:5){
set.seed(123)
print(i)
train_data = data[-unlist(order[i]),] %>% as.data.frame()
test_data = data[unlist(order[i]),] %>% as.data.frame()
pred_name = colnames(train_data)[ncol(train_data)]
train_file <- paste0(tempdir(), "/train.csv")
write.csv(train_data, train_file, row.names = FALSE)
test_file_to_eval <- paste0(tempdir(), "/eval.csv")
write.csv(test_data, test_file_to_eval, row.names = FALSE)
devtools::install()
install.packages("C:/Users/ft7b6/OneDrive/Desktop/Quantlyzer", repos = NULL, type="source")
library(Quantlyzer)
data2 <- read_csv("C:/Users/ft7b6/Downloads/Data_v7_220503_clean2.csv")
colnames(data2) <- paste0('a', colnames(data2))
a2_start = Sys.time()
a2 <- stat_ml(data2,
pos_X = c(7, 10, 11, 12,13,14, 19,20,21:377),
pos_y = 8,
objective = 'reg',
na_method = 'omit',
scale_method = 'scale'
)
a2_end = Sys.time()
b2
pearson_cor = c()
r_2 = c()
rmseee = c()
maeee = c()
set.seed(123)
oooorder = sample(1:nrow(data), floor(0.75 * nrow(data)), replace = FALSE)
train_data = data[oooorder,]
train_label = train_data[,ncol(train_data)]
train_data = train_data[,-ncol(train_data)]
test_data = data[-oooorder,]
test_label = test_data[,ncol(test_data)]
test_data = test_data[,-ncol(test_data)]
trans_model = preProcess(train_data, method = trans_method)
train_data = predict(trans_model, train_data) %>% as.matrix()
test_data = predict(trans_model, test_data)  %>% as.matrix()
data = data_selection(data2,
pos_X = c(7, 10, 11, 12,13,14, 19,20,21:377),
pos_y = 8,
'omit')
order = data_into_five(data)
data
trans_method = 'scale'
pearson_cor = c()
r_2 = c()
rmseee = c()
maeee = c()
set.seed(123)
oooorder = sample(1:nrow(data), floor(0.75 * nrow(data)), replace = FALSE)
train_data = data[oooorder,]
train_label = train_data[,ncol(train_data)]
train_data = train_data[,-ncol(train_data)]
test_data = data[-oooorder,]
test_label = test_data[,ncol(test_data)]
test_data = test_data[,-ncol(test_data)]
trans_model = preProcess(train_data, method = trans_method)
train_data = predict(trans_model, train_data) %>% as.matrix()
test_data = predict(trans_model, test_data)  %>% as.matrix()
print(i)
set.seed(123)
print(i)
train_data = data[-unlist(order[i]),] %>% as.data.frame()
test_data = data[unlist(order[i]),] %>% as.data.frame()
pred_name = colnames(train_data)[ncol(train_data)]
train_file <- paste0(tempdir(), "/train.csv")
write.csv(train_data, train_file, row.names = FALSE)
test_file_to_eval <- paste0(tempdir(), "/eval.csv")
write.csv(test_data, test_file_to_eval, row.names = FALSE)
class <- model_structured_data_regressor(max_trials = 20) %>%
fit(
train_file,
pred_name,
validation_data = list(test_file_to_eval, pred_name)
)
pred <- class %>% predict(test_file_to_eval)
true = test_data[,ncol(test_data)] %>% as.numeric()
pearson_cor <- c(pearson_cor, cor(pred, true))
r_2 <- c(r_2, coefficient_of_determination(true, pred))
rmseee <- c(rmseee, rmse(true, pred))
maeee <- c(maeee, mae(true, pred))
paste('Pearson Coefficient Correlation :', pearson_cor) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('Coefficient of Determination :', r_2) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('RMSE : ', rmseee) %>% print()
print("      ")
print(" __________________________ ")
print("      ")
paste('MAE : ', maeee) %>% print()
overall <- list(pearson_cor, r_2, rmseee, maeee)
rm(list=ls())
library("e1071")
library("caret")
library("dplyr")
library("robustHD")
img <- readxl::read_excel("C:/Users/ft7b6/Downloads/data22.xlsx")
a <- img %>% filter(Field == "6_1")
b <- img %>% filter(Field == "6_3")
c <- img %>% filter(Field == "8_1")
set.seed(12345)
data  <- img[,c(4,6,8,10,12,14,16,18)]
new_img <- data
new_img[which(new_img$mean_D2 <= 2.2),]$mean_D2 = 1
new_img[which(new_img$mean_D2 < 3 & new_img$mean_D2 > 2.2),]$mean_D2 = 2
new_img[which(new_img$mean_D2 < 5 & new_img$mean_D2 >= 3),]$mean_D2 = 3
img <- new_img
img$mean_D2 <- floor(img$mean_D2)
data <- img
data$mean_D2 %>% table()
install.packages("C:/Users/ft7b6/OneDrive/Desktop/Quantlyzer", repos = NULL, type="source")
rm(list=ls())
library("e1071")
library("caret")
library("dplyr")
library("robustHD")
img <- readxl::read_excel("C:/Users/ft7b6/Downloads/data22.xlsx")
a <- img %>% filter(Field == "6_1")
b <- img %>% filter(Field == "6_3")
c <- img %>% filter(Field == "8_1")
set.seed(12345)
data  <- img[,c(4,6,8,10,12,14,16,18)]
new_img <- data
new_img[which(new_img$mean_D2 <= 2.2),]$mean_D2 = 1
new_img[which(new_img$mean_D2 < 3 & new_img$mean_D2 > 2.2),]$mean_D2 = 2
new_img[which(new_img$mean_D2 < 5 & new_img$mean_D2 >= 3),]$mean_D2 = 3
############### We need to normalize the data#########################
img <- new_img
img$mean_D2 <- floor(img$mean_D2)
data <- img
data$mean_D2 %>% table()
install.packages("C:/Users/ft7b6/OneDrive/Desktop/Quantlyzer", repos = NULL, type="source")
library(Quantlyzer)
library(devtools)
install_github('tianfengkai/Quantlyzer')
